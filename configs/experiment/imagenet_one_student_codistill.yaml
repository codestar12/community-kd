# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: hydra_com.yaml
  - override /datamodule: shard_imagenet.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /callbacks: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "hydra_codistill"

seed: 12345

trainer:
  # uncomment for very fast testing batches
  # limit_train_batches: 0.1
  # limit_val_batches: 0.1
  min_epochs: 1
  max_epochs: 100
  # when using webdataset this needs to be provided manually
  # should be floor(dataset size / batch size / num gpus) + 1
  # thats close enough for government work
  # total train batchs at 128 10009.1171875
  # total test batches at 128 390.625
  # 3 gpus batchsize 128
  limit_train_batches: 3337
  limit_val_batches: 131
  # 2 gpus batchsize 128
  # limit_train_batches:  5005
  # limit_val_batches: 196
  # 1 gpus batchsize 128
  # limit_train_batches: 10010
  # limit_val_batches: 391
  precision: 16
  check_val_every_n_epoch: 1
  accelerator: gpu
  strategy:
    _target_: pytorch_lightning.strategies.DDPStrategy
    find_unused_parameters: False
  devices:
    - 1
    - 2
    - 3

datamodule:
  batch_size: 256
  pin_memory: False
  num_workers: 4
  train_size: 160
  test_size: 224

model:
  num_classes: 1000
  lr_milestones: [100, 180]
  lr: 0.008
  weight_decay: 0.02
  num_students: 1
  student_layers: [[1, 1]]
  warmup: 5
  kd_delay: 0
  kd_trans_epochs: 0
  hard_label_end: 0.2

  kd_weights:
    - [0.0, 0.1]
    - [0.9, 0.0]

callbacks:
  early_stopping:
    monitor: "teacher/val/acc"
    patience: 100

  model_checkpoint:
    monitor: "teacher/val/acc"

logger:
  wandb:
    log_model: True
